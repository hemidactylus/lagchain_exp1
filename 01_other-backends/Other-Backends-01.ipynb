{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee33d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9441141",
   "metadata": {},
   "source": [
    "### In-mem cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca15ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the caching really obvious, lets use a slower model.\n",
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13039149",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30df1d6",
   "metadata": {},
   "source": [
    "### SQLite cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm .langchain.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same thing with a SQLite cache\n",
    "from langchain.cache import SQLiteCache\n",
    "langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffac825",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115080eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Not exact match, should take long again\n",
    "llm(\"Tell me a nice joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a84d056",
   "metadata": {},
   "source": [
    "### RedisSemanticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e129b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.cache import RedisSemanticCache\n",
    "\n",
    "\n",
    "langchain.llm_cache = RedisSemanticCache(\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8532f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The second time, while not a direct hit, the question is semantically similar to the original question,\n",
    "# so it uses the cached result!\n",
    "llm(\"Tell me one joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit playing with the embeddings\n",
    "import numpy as np\n",
    "\n",
    "emb = langchain.llm_cache.embedding\n",
    "\n",
    "pv1 = np.array(emb.embed_query(\"Tell me a joke\"), dtype=float)\n",
    "pv2 = np.array(emb.embed_query(\"Tell me one joke\"), dtype=float)\n",
    "pvZ = np.array(emb.embed_query(\"I once saw a platypus cooking dinner\"), dtype=float)\n",
    "\n",
    "print('pv1 . pv1 = %.4f', np.dot(pv1, pv1))\n",
    "print('pv2 . pv2 = %.4f', np.dot(pv2, pv2))\n",
    "print('pvZ . pvZ = %.4f', np.dot(pvZ, pvZ))\n",
    "print('')\n",
    "print('pv1 . pv2 = %.4f', np.dot(pv1, pv2))\n",
    "print('pv1 . pvZ = %.4f', np.dot(pv1, pvZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e1b3f",
   "metadata": {},
   "source": [
    "### GPTCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5cc11",
   "metadata": {},
   "source": [
    "#### Exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf96ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gptcache\n",
    "from gptcache.processor.pre import get_prompt\n",
    "from gptcache.manager.factory import get_data_manager\n",
    "from langchain.cache import GPTCache\n",
    "\n",
    "# Avoid multiple caches using the same file, causing different llm model caches to affect each other\n",
    "i = 0\n",
    "file_prefix = \"data_map\"\n",
    "\n",
    "def init_gptcache_map(cache_obj: gptcache.Cache):\n",
    "    global i\n",
    "    cache_path = f'{file_prefix}_{i}.txt'\n",
    "    cache_obj.init(\n",
    "        pre_embedding_func=get_prompt,\n",
    "        data_manager=get_data_manager(data_path=cache_path),\n",
    "    )\n",
    "    i += 1\n",
    "\n",
    "langchain.llm_cache = GPTCache(init_gptcache_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68104527",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The second time it is, so it goes faster\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe36a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some inspection to play with this gptcache hidden in the belly of the langchain stuff\n",
    "gpCache = langchain.llm_cache.gptcache_dict[\"[('_type', 'openai'), ('best_of', 2), ('frequency_penalty', 0), ('logit_bias', {}), ('max_tokens', 256), ('model_name', 'text-davinci-002'), ('n', 2), ('presence_penalty', 0), ('request_timeout', None), ('stop', None), ('temperature', 0.7), ('top_p', 1)]\"]\n",
    "# Force file write :)\n",
    "gpCache.flush()\n",
    "\n",
    "# In this case you can check that...\n",
    "gpcache = list(langchain.llm_cache.gptcache_dict.values())[0]\n",
    "gpcache.embedding_func('aaa')\n",
    "# ... this is the identity (exact cache!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1435f237",
   "metadata": {},
   "source": [
    "#### Similarity caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3227e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gptcache\n",
    "from gptcache.processor.pre import get_prompt\n",
    "from gptcache.manager.factory import get_data_manager\n",
    "from langchain.cache import GPTCache\n",
    "from gptcache.manager import get_data_manager, CacheBase, VectorBase\n",
    "from gptcache import Cache\n",
    "from gptcache.embedding import Onnx\n",
    "from gptcache.similarity_evaluation.distance import SearchDistanceEvaluation\n",
    "\n",
    "# Avoid multiple caches using the same file, causing different llm model caches to affect each other\n",
    "i = 0\n",
    "file_prefix = \"data_map\"\n",
    "llm_cache = Cache()\n",
    "\n",
    "\n",
    "def init_gptcache_map(cache_obj: gptcache.Cache):\n",
    "    global i\n",
    "    cache_path = f'{file_prefix}_{i}.txt'\n",
    "    onnx = Onnx()\n",
    "    cache_base = CacheBase('sqlite')\n",
    "    vector_base = VectorBase('faiss', dimension=onnx.dimension)\n",
    "    data_manager = get_data_manager(cache_base, vector_base, max_size=10, clean_size=2)\n",
    "    cache_obj.init(\n",
    "        pre_embedding_func=get_prompt,\n",
    "        embedding_func=onnx.to_embeddings,\n",
    "        data_manager=data_manager,\n",
    "        similarity_evaluation=SearchDistanceEvaluation(),\n",
    "    )\n",
    "    i += 1\n",
    "\n",
    "langchain.llm_cache = GPTCache(init_gptcache_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what the similarity cache uses under the hood:\n",
    "from gptcache.embedding import Onnx\n",
    "Onnx().to_embeddings('Today is a sunny day.')\n",
    "# a (768,) ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb20ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74923f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This is an exact match, so it finds it in the cache\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This is not an exact match, but semantically within distance so it hits!\n",
    "llm(\"Tell me joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b8946a",
   "metadata": {},
   "source": [
    "### SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use SQLAlchemyCache to cache with any SQL database supported by SQLAlchemy.\n",
    "\n",
    "from langchain.cache import SQLAlchemyCache\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:cachepwd@172.17.0.2:5432/postgres\")\n",
    "langchain.llm_cache = SQLAlchemyCache(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b6bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This is an exact match, so it finds it in the cache\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7353915",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This is not an exact match, but semantically within distance so it hits!\n",
    "llm(\"Tell me joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be347d8b",
   "metadata": {},
   "source": [
    "##### SQLAlchemy with custom schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb325a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can define your own declarative SQLAlchemyCache child class to customize the schema used for caching. For example, to support high-speed fulltext prompt indexing with Postgres, use:\n",
    "\n",
    "from sqlalchemy import Column, Integer, String, Computed, Index, Sequence\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy_utils import TSVectorType\n",
    "from langchain.cache import SQLAlchemyCache\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class FulltextLLMCache(Base):  # type: ignore\n",
    "    \"\"\"Postgres table for fulltext-indexed LLM Cache\"\"\"\n",
    "\n",
    "    __tablename__ = \"llm_cache_fulltext\"\n",
    "    id = Column(Integer, Sequence('cache_id'), primary_key=True)\n",
    "    prompt = Column(String, nullable=False)\n",
    "    llm = Column(String, nullable=False)\n",
    "    idx = Column(Integer)\n",
    "    response = Column(String)\n",
    "    prompt_tsv = Column(TSVectorType(), Computed(\"to_tsvector('english', llm || ' ' || prompt)\", persisted=True))\n",
    "    __table_args__ = (\n",
    "        Index(\"idx_fulltext_prompt_tsv\", prompt_tsv, postgresql_using=\"gin\"),\n",
    "    )\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:cachepwd@172.17.0.2:5432/postgres\")\n",
    "langchain.llm_cache = SQLAlchemyCache(engine, FulltextLLMCache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490a715",
   "metadata": {},
   "source": [
    "Note: this uses (not investigated too much) the [TSVECTOR](https://docs.sqlalchemy.org/en/20/dialects/postgresql.html#full-text-search) Postgres type which should be the basis for a full-text search (does not seem to relate to \"vector search\", however. Indeed, out of the box, this is not semantical similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a964cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This is not an exact match, but semantically within distance so it hits!\n",
    "llm(\"Tell me joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c513f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This is not an exact match, but semantically within distance so it hits!\n",
    "llm(\"Tell me a joke\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
